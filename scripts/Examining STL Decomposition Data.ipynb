{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5bf659-2df4-46ab-a84b-e4a8e5499ad2",
   "metadata": {},
   "source": [
    "This notebook takes < ?mins to run top-to-bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a817ef2-19d5-4d38-ba4b-50084f2153b3",
   "metadata": {},
   "source": [
    "# This is a notebook for examining data after the ensemble numpy arrays (shape: [8,456,392,504]) are decomposed into trend, seasonal and residual components giving an array of shape ([8,3,456,392,504])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22667a2b-f247-4a92-8b6e-daafcf463043",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Description of ensemble array data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ebddf-6b46-4e9c-805f-44df1b1a615a",
   "metadata": {},
   "source": [
    "See '/scripts/Examining Ensemble Numpy Arrays' to experiment with the ensemble array data.\n",
    "\n",
    "General Features:\n",
    "- There is an array per variable, with each array having shape [8,456,392,504].\n",
    "- The first dimension is the model, there are 8 models corresponding to: ['ERAI','ERA5','MetUM(044)','MetUM(011)','MAR(ERAI)','MAR(ERA5)', 'RACMO(ERAI)','RACMO(ERA5)']\n",
    "- The second dimension is the time dimension, it goes from 1981-2018 in monthly increments.\n",
    "- The third and forth dimensions are the grid latitude/longitude dimensions.\n",
    "- Values for snowfall and melt are in kg/m^2, while values for temperature are in Kelvin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e2c13-1872-4775-b41b-bb6af0853cb1",
   "metadata": {},
   "source": [
    "## Description of decomposing into trend, seasonal and residual components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395016df-cb62-4bd3-b55a-146e687b39fb",
   "metadata": {},
   "source": [
    "See '/scripts/stl_decomposition_on_numpy_ensemble_arrays.py' for the code used.\n",
    "\n",
    "General Process:\n",
    "- The [8,456,392,504] arrays are loaded and dask is used with chunks of size [8,456,28,28].\n",
    "- STL decomposition is then applied across the array to every timeseries, decomposing the 456 length series into 3 456 length components, corresponding to the trend, seasonal and residual components.\n",
    "- A seasonal smoother of n_s = 13 is used.\n",
    "- The final [8,3,456,392,504] array for each variable is saved as an approximately 15GB numpy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c7e10-4968-42b7-852e-13f74a62e690",
   "metadata": {},
   "source": [
    "## Examining STL Decomposition Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af690ea-82b0-48be-ac88-30f034fec3a7",
   "metadata": {},
   "source": [
    "Due to difficulties loading the 15GB files instead of display results here I refer the reader to the notebooks used to create figures for the paper including: Paper Figures and Tables, Appendix Figures and Tables and Supplementary Figures and Tables. (Note while I have saved files as numpy arrays I would recommend using an alternative approach with a file type more suited to large memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cordex-analysis-env",
   "language": "python",
   "name": "cordex-analysis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
